\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{cmap}  % should be before fontenc
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{unicode-math}
\usepackage[pdftex,colorlinks=true,linkcolor=blue,urlcolor=red,unicode=true,hyperfootnotes=false,bookmarksnumbered]{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{indentfirst}

\newcommand{\E}{\ensuremath{\mathsf{E}}}  
\newcommand{\D}{\ensuremath{\mathsf{D}}}  
\newcommand{\Prb}{\ensuremath{\mathsf{P}}}  
\newcommand{\eps}{\varepsilon}  
\renewcommand{\phi}{\varphi} 
\renewcommand{\le}{\leqslant}  
\renewcommand{\leq}{\leqslant}  
\renewcommand{\ge}{\geqslant}  
\renewcommand{\geq}{\geqslant}  
\renewcommand\qedsymbol{Q.E.D.}

\newtheorem{theorem}{Теорема} 
\newtheorem{lemma}{Лемма}
\newtheorem{consequence}{Следствие} 

\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]
\newcommand{\question}{\textbf{Вопрос аудитории: }}  

\theoremstyle{remark}
\newtheorem*{remark}{Примечание}

\Crefformat{lemma}{#2Lemma#3}   
\crefformat{lemma}{#2lemma#3}

\pagestyle{myheadings}
\markright{Probablity Theory. SMS 2024. Greg Sosnovtsev\hfill}  

\begin{document}

\tableofcontents


\section{День 0*. Прелюдия, повторяем комбинаторику}

\section{День 1. Случайные события и элементарное определение вероятности}

Назовем \textit{множеством элементарных исходов} $\varOmega = \{\omega_1\omega_2,...\omega_n \}$ такое конечное множеством, 
что $\omega_i$ и $\omega_j$ несовместны

Тогда \textit{Событие} - это любое множество элементарных исходов. 

Пусть $A \subset \varOmega$ - событие, тогда
\begin{itemize}
    \item $ A \cup B = \{\omega \in \varOmega : \omega \in A$ или $ \omega \in B\} $ 
    \item  $ A \cap B = \{\omega \in \varOmega : \omega \in A$ и $ \omega \in B\} $ 
    \item $ \overline{A} = \{ \omega \in \varOmega : \omega \notin A \} $
\end{itemize}

\begin{remark}

Рассмотрим эксперимент бросок кубика d6 

$\varOmega = \{1,2,3,4,5,6\}$

Событие, что выпала грань с четным числом - \{2,4,6\}
Событие, что выпала грань с числом меньше 3 - \{1,2\}

\end{remark}

Определим некоторую функцию $P: \varOmega \to [0,1]$, которую назовем 
\textit{распределением вероятностей}
Такую, что 
$$
\sum_{\omega \in \varOmega} P(\omega)  = 1
$$

Вероятность P(A) \textit{события} A тогда определим как 
$$
P(A) = \sum_{\omega \in A} P(\omega)  
$$

Пару $(\varOmega, P)$ будем называть \textit{Дискретным вероятностным пространством}



Примитивные свойства:

\begin{enumerate}
 \item $ P(\varnothing) = 0 $
 \item $ P(\varOmega) = 1 $
 \item $P(\overline{A}) = 1 - P(A)$
 \item Если A и B \textit{не совместны} то есть $A \cap B = \varnothing$ то $ P(A \cup B) = P(A) + P(B) $
 \item $ P(A \cup B) = P(A) + P(B) - P(A \cap B) $
\end{enumerate}

\question{Верно ли что нулевая вероятность может быть только у пустого события ($\varnothing$) ?}

\begin{theorem} [Формула включений-исключений]
    $$ 
    P(\bigcup\limits_{i=1}^{m} A_i) = \sum_{1}^{m} P(A_i) - \sum_{i<j} P(A_i \cap A_j) + \sum_{i<j<k} P(A_i \cap A_j \cap A_k) ... + (-1)^{m-1} P(\bigcap\limits_{i=1}^{m} A_i)
    $$
\end{theorem}
\begin{proof}
    Будем вести доказательство индукцией по m

    База при m = 1 очевидна, при m = 2 сошлемся на пятый пункт, который также очевиден.

    Переход от m к m+1:

    Пусть $B = \bigcup\limits_{i=1}^{m} A_i$

    $$
    P(\bigcup\limits_{i=1}^{m+1} A_i) = P(B \cup A_{m+1}) = P(B) + P(A_{m+1}) - P(B \cap A_{m+1})
    $$
    Пусть $B_i = A_i \cap A_{m+1} \Rightarrow P(B \cap A_{m+1}) = P(\bigcup\limits_{i=1}^{m} B_i)$
    $$
    \Rightarrow P(B) + P(A_{m+1}) - P(B \cap A_{m+1}) = P(B) + P(A_{m+1}) - P(\bigcup\limits_{i=1}^{m} B_i) 
    $$

    $$
    = (\sum_{1}^{m} P(A_i)  - \sum_{i<j \le m} P(A_i \cap A_j)  + \sum_{i<j<k \le m} P(A_i \cap A_j \cap A_k) - ... ) + P(A_{m+1}) 
    $$
    $$
    - (\sum_{1}^{m} P(A_i \cap A_{m+1})  - \sum_{i<j \le m} P(A_i \cap A_j  \cap A_{m+1})  + ... )
    $$

    Остается сгруппировать и понять, что это именно то, что нас интересует.
\end{proof}

Рассмотрим важный частный случай, когда все элементарные исходы равновозможны, 
то есть $P(\omega_i) = \frac{1}{|\varOmega|}$

Тогда вероятность события A равна
$$
P(A) = \frac{|A|}{|\varOmega|}
$$


\section{День 2. Условная вероятность и независимые события}


\question{Предположим Степан ученик 11 класса и сдает 3 предмета в этом году, Степан имеет дома обширную коллекцию книг,
 а в 8 классе районной библиотеке он даже получил награду "Чтец года", как вы думаете 
 какой предмет будет более вероятно сдавать Степан в этом году информатику (примечание для преподавателя 21\% на 2024) 
 или литературу (примечание для преподавателя 8\% на 2024)? }

В этом разделе я предлагаю отойти от чуть более общепринятых обозначений A и B для событий и использовать 
обозначения H (hypothesis) и E (evidence)  


\subsection{Условная вероятность}

Пусть $P(E) > 0$ 

Тогда Вероятность события H \textit{при условии} что наступило событие E определим как
$$
P(H | E) = \frac{P(H \cap E)}{P(E)}
$$

То есть мы сужаем наше вероятностное пространство до тех событий которые удовлетворяют некоторому факту

Свойства 
\begin{enumerate}
    \item $P(H | H) = 1$ и если $E \subset H$ то $P(H | E) = 1$
    \item $P(\varnothing | E) = 0$
    \item Если $H_1$ и $H_2$ не совместны, то $P(H_1 \cup H_2 | E) = P(H_1 | E) + P(H_2 | E)$
    \item $P(\overline{H} | E) + P(H | E) = 1$
\end{enumerate}

\question{Будет ли равняться 1 выражение $P(H | E) + P(H | \overline{E})$}

\subsection{Теорема Байеса}

\begin{lemma} [Формула Байеса]
    $$
    P(H | E) = \frac{P(H)P(E | H)}{P(E)}
    $$
\end{lemma}
\begin{proof}
    $$
    P(H | E)P(E) = P(E \cap H) = P(H)P(E | H)
    $$
\end{proof}

Формула Байеса позволяет проверять верность некоторой гипотезы при все новых поступающих фактах обрисовывающих
контекст текущего эксперимента и позволяют все лучше подбирать модель под данную доменную область, будь то естественные науки
или машинное обучение, в свое время байесовские методы достаточно хорошо продвинули данную область.

Можно также расширить данную формулу до полноценной теоремы, если рассуждать о некотором наборе несовместных гипотез:

Пусть есть разбиение множества элементарных исходов на несовместные события $\varOmega = \bigsqcup\limits_{i=1}^n H_i$
и для каждого события верно $P(H_i) > 0$

\begin{theorem}[Формула полной вероянтости]
    Тогда иммеет место быть формула: 
    $$
    P(E) = \sum_{i=1}^{n} P(E | H_i)P(H_i)
    $$
\end{theorem}
\begin{proof}
    Пусть $E_i = E \cap H_i$
    Тогда $E = E \cap \varOmega = E \cap \bigsqcup\limits_{i=1}^n H_i = \bigsqcup\limits_{i=1}^n (E \cap H_i) = \bigsqcup\limits_{i=1}^n E_i$
    - Разбиение событие A на несовместные события $E_i$ 
    Также заметим, что из Формулы Байеса $P(E_i) = P(E \cap H_i) = P(E | H_i)P(H_i)$ 
    $$
    \Rightarrow P(E) = \sum_{i=1}^{n} P(E_i) = \sum_{i=1}^{n} P(E | H_i)P(H_i)
    $$
\end{proof}

Формула же полной вероятности позволяет нам действовать в обратную сторону и предполагая, что мы имеем некоторую достаточно достоверную модель
делать предположения о вероятности наступления некоторого явления в будущем.

\begin{theorem}[Теорема Байеса]
    Также имеем 
    $$
    P(H_i | E) = \frac{P(E | H_i)P(H_i)}{\sum_{j=1}^{n} P(E | H_j)P(H_j)}
    $$
\end{theorem}
\begin{proof}
    Из определения имеем 
    $$
    P(E \cap H_i) = P(E | H_i)P(H_i)
    $$
    Из формулы полной вероятности имеем 
    $$
    P(E) = \sum_{j=1}^{n} P(E | H_j)P(H_j)
    $$
    Подставим полученные выше значения в определение условной вероятности для $P(H_i | E) = \frac{P(E \cap H_i)}{P(E)}$
    И получим как раз то что нам нужно
\end{proof}

\subsection{Независимые события}

Назовем события A и B \textit{независимыми} если имеет место быть $P(A \cap B) = P(A)P(B)$
И, как следует из семантики слова, верно следующее $P(A | B) = P(A)$ и $P(B | A) = P(B)$

Также заметим, что из независимости A и B следует независимость A и $\overline{B}$
\begin{proof}
    $$
    P(A \cap \overline{B}) + P(A \cap B) = P(A)
    \Rightarrow P(A \cap \overline{B}) = P(A) - P(A \cap B)
    $$
    $$
    = P(A) - P(A)P(B) = P(A)(1 - P(B)) = P(A)P(\overline{B})
    $$
\end{proof}

\question{Верно ли что если $A \cap B = \varnothing$, то события независисмы? }

В качестве домашнего задания подумать о Monty Hall problem на языке на котором мы сегодня говорили.

\section{День 3. Случайные величины и вероятностные характеристики}

\subsection{Случайная величина и ее распределение}

Пусть имеется дискретное вероятностное пространство $(\varOmega, P)$

Назовем \textit{случайной величиной} некоторый функционал $\xi: \varOmega \to \mathbb{R}$.

То есть случайные величины это некоторые численные характеристики, которыми обладает наша модель заданная
некоторым вероятностным пространством. Например, эксперимента с подрасыванием монетки случайной величиной будет
количество выпавших за эксперимент орлов.


Также пусть $X = \xi(\varOmega)$ - \textit{множество значений} случайной величины $\xi$ 
В таком случае можем рассматривать события вида 
$$
A_x = {\omega \in varOmega: \xi(\omega) = x}
$$
Тем самым мы получим распределение вероятнотей на множестве X.
Обозначим за $P_\xi(x) = P(A_x)$


Легко заметить, что 
$$
\sum_{x \in X} P_\xi(x)  = 1
$$

Но тогда из введенных нами определений следует, что пара $(\varOmega, P_\xi)$ 
представляет из себя ничто иное, как \textit{дискретное вероятностное пространство}.
А функция $P_\xi$ называется \textit{распределением случайной величины} $\xi$

Весьма логично также будет ввести понятие независимости для случайных величин 

Пускай $\xi_1, \xi_2$ - случайные величины, тогда скажем, что они \textit{независимые}, 
если

$$
\forall x_1, x_2 \in X \Rightarrow P(\{\omega \in \varOmega: \xi_1(\omega) = x_1, \xi_2(\omega) = x_2\}) = P_{\xi_1}(x_1)P_{\xi_2}(x_2)
$$



\section{День 4. Геометрическая вероятность и Метод Монте-Карло}

\section{День 5. Эпилог, что дальше?}

Вероятностные методы решения комбинаторных задач и задач из теории графов? ЦПТ? 

\section{День 6. Зачет}

\section*{Post Scriptum}

\end{document}